{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4808b7",
   "metadata": {},
   "source": [
    "## Spark-DataFrame  Fundamental Practice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2b2e6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f071f062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the PySpark environment veriable\n",
    "os.environ['JAVA_HOME']=\"C:\\Program Files\\Java\\jdk-22\"\n",
    "os.environ['SPARK_HOME'] = \"E:\\spark-3.5.1-bin-hadoop3\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"jupyter\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPS'] = \"notebook\"\n",
    "os.environ['PYSPARK_PYTHON'] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "67057373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sart the Sparksession\n",
    "spark = SparkSession.builder.appName(\"Practice\")\\\n",
    ".config(\"spark.executor.memory\",\"4g\")\\\n",
    ".config(\"spark.sql.shuffle.partitions\",\"4\")\\\n",
    ".config(\"spark.jars\",\"E:\\jdbc-driver\\postgresql-42.7.3.jar\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "04952b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e6ce2048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BE50MUO:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Practice>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9141a1",
   "metadata": {},
   "source": [
    "### 1. Creating DataFrame and Loading CSV, Text, Json,  Parquet File Into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e2b3458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "data = [\n",
    "  ('James','Smith','1991-04-01','M',3000),\n",
    "  ('Michael','Rose','2000-05-19','M',4000),\n",
    "  ('Robert','Williams','1978-09-05','M',4000),\n",
    "  ('Maria','Jones','1967-12-01','F',4000),\n",
    "  ('Jen','Brown','1980-02-17','F',8500),\n",
    "  ('Keith','Lina','1982-09-26','F',8000)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "pyspark_df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c391305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show the list of Schema\n",
    "pyspark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "59e9c0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+------+------+\n",
      "|firstname|lastname|       dob|gender|salary|\n",
      "+---------+--------+----------+------+------+\n",
      "|    James|   Smith|1991-04-01|     M|  3000|\n",
      "|  Michael|    Rose|2000-05-19|     M|  4000|\n",
      "|   Robert|Williams|1978-09-05|     M|  4000|\n",
      "|    Maria|   Jones|1967-12-01|     F|  4000|\n",
      "|      Jen|   Brown|1980-02-17|     F|  8500|\n",
      "|    Keith|    Lina|1982-09-26|     F|  8000|\n",
      "+---------+--------+----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show data values\n",
    "pyspark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "33b20e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n",
      "|   Name| Departments|salary|\n",
      "+-------+------------+------+\n",
      "| chandu|Data Science| 10000|\n",
      "| chandu|         IOT|  5000|\n",
      "| Rohith|    Big Data|  4000|\n",
      "| chandu|    Big Data|  4000|\n",
      "| Rohith|Data Science|  3000|\n",
      "|krishna|Data Science| 20000|\n",
      "|krishna|         IOT| 10000|\n",
      "|krishna|    Big Data|  5000|\n",
      "| rashmi|Data Science| 10000|\n",
      "| rashmi|    Big Data|  2000|\n",
      "+-------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read git from csv and store pysparkDataFrame\n",
    "url_github = r\"https://raw.githubusercontent.com/muttinenisairohith/Datasets/b0bb96f293adbb803e24c26b7780e078372d3703/data/test2.csv\"\n",
    "pd_df = pd.read_csv(url_github)\n",
    "df_pyspark1 = spark.createDataFrame(pd_df)\n",
    "df_pyspark1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "13e384e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|Data Science|      43000|\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#group by Departments which gives summation of salaries\n",
    "df_pyspark1.groupBy(\"Departments\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "de5ea747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(salary)|\n",
      "+------------+-----------+\n",
      "|Data Science|      43000|\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1.groupBy(\"Departments\").agg(({\"salary\":\"sum\"})).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3c3b782b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|min(salary)|\n",
      "+------------+-----------+\n",
      "|Data Science|       3000|\n",
      "|         IOT|       5000|\n",
      "|    Big Data|       2000|\n",
      "+------------+-----------+\n",
      "\n",
      "+------------+-----------+\n",
      "| Departments|max(salary)|\n",
      "+------------+-----------+\n",
      "|Data Science|      20000|\n",
      "|         IOT|      10000|\n",
      "|    Big Data|       5000|\n",
      "+------------+-----------+\n",
      "\n",
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|Data Science|    10750.0|\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "+------------+-----------+\n",
      "\n",
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|Data Science|    10750.0|\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark1.groupBy(\"Departments\").min(\"salary\").show()\n",
    "df_pyspark1.groupBy(\"Departments\").max(\"salary\").show()\n",
    "df_pyspark1.groupBy(\"Departments\").avg(\"salary\").show()\n",
    "df_pyspark1.groupBy(\"Departments\").mean(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4845f1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "|age|gender|            name|course|  roll|marks|               email|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|  2984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud| 12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF| 21267|   45|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB| 32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA| 41487|   41|Claude Panos_Judi...|\n",
      "| 28|  Male|  Margene Moores|   MVC| 52771|   32|Toshiko Hillyard_...|\n",
      "| 28|  Male|     Neda Briski|   OOP| 61973|   69|Alberta Freund_El...|\n",
      "| 28|Female|    Claude Panos| Cloud| 72409|   85|Sheryll Towler_Al...|\n",
      "| 28|  Male|  Celeste Lollis|   MVC| 81492|   64|Nicole Harwood_Cl...|\n",
      "| 29|  Male|  Cordie Harnois|   OOP| 92882|   51|Judie Chipps_Clem...|\n",
      "| 29|Female|       Kena Wild|   DSA|102285|   35|Dustin Feagins_Ma...|\n",
      "| 29|  Male| Ernest Rossbach|    DB|111449|   53|Maybell Duguay_Ab...|\n",
      "| 28|Female|  Latia Vanhoose|    DB|122502|   27|Latia Vanhoose_Mi...|\n",
      "| 29|Female|  Latia Vanhoose|   MVC|132110|   55|Eda Neathery_Nico...|\n",
      "| 29|  Male|     Neda Briski|    PF|141770|   42|Margene Moores_Mi...|\n",
      "| 29|Female|  Latia Vanhoose|    DB|152159|   27|Claude Panos_Sant...|\n",
      "| 29|  Male|  Loris Crossett|   MVC|161771|   36|Mitzi Seldon_Jenn...|\n",
      "| 29|  Male|  Annika Hoffman|   OOP|171660|   22|Taryn Brownlee_Mi...|\n",
      "| 29|  Male|   Santa Kerfien|    PF|182129|   56|Judie Chipps_Tary...|\n",
      "| 28|Female|Mickey Cortright|    DB|192537|   62|Ernest Rossbach_M...|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using SparkFiles Load the dataframe\n",
    "from pyspark import SparkFiles\n",
    "student_info = r\"https://raw.githubusercontent.com/AISCIENCES/course-master-big-data-with-pyspark-and-aws/main/Code/03-Spark%20DFs/StudentData.csv\"\n",
    "spark.sparkContext.addFile(student_info)\n",
    "student_df = spark.read.csv(SparkFiles.get(\"StudentData.csv\"),inferSchema=True, header=True)\n",
    "student_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "77168b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(student_df.count())\n",
    "print(len(student_df.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9eb81ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "|age|gender|            name|course|  roll|marks|               email|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|  2984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud| 12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF| 21267|   45|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB| 32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA| 41487|   41|Claude Panos_Judi...|\n",
      "| 28|  Male|  Margene Moores|   MVC| 52771|   32|Toshiko Hillyard_...|\n",
      "| 28|  Male|     Neda Briski|   OOP| 61973|   69|Alberta Freund_El...|\n",
      "| 28|Female|    Claude Panos| Cloud| 72409|   85|Sheryll Towler_Al...|\n",
      "| 28|  Male|  Celeste Lollis|   MVC| 81492|   64|Nicole Harwood_Cl...|\n",
      "| 29|  Male|  Cordie Harnois|   OOP| 92882|   51|Judie Chipps_Clem...|\n",
      "| 29|Female|       Kena Wild|   DSA|102285|   35|Dustin Feagins_Ma...|\n",
      "| 29|  Male| Ernest Rossbach|    DB|111449|   53|Maybell Duguay_Ab...|\n",
      "| 28|Female|  Latia Vanhoose|    DB|122502|   27|Latia Vanhoose_Mi...|\n",
      "| 29|Female|  Latia Vanhoose|   MVC|132110|   55|Eda Neathery_Nico...|\n",
      "| 29|  Male|     Neda Briski|    PF|141770|   42|Margene Moores_Mi...|\n",
      "| 29|Female|  Latia Vanhoose|    DB|152159|   27|Claude Panos_Sant...|\n",
      "| 29|  Male|  Loris Crossett|   MVC|161771|   36|Mitzi Seldon_Jenn...|\n",
      "| 29|  Male|  Annika Hoffman|   OOP|171660|   22|Taryn Brownlee_Mi...|\n",
      "| 29|  Male|   Santa Kerfien|    PF|182129|   56|Judie Chipps_Tary...|\n",
      "| 28|Female|Mickey Cortright|    DB|192537|   62|Ernest Rossbach_M...|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping rows based on null values\n",
    "student_df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a052a009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>course</th>\n",
       "      <th>roll</th>\n",
       "      <th>marks</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hubert Oliveras</td>\n",
       "      <td>DB</td>\n",
       "      <td>2984</td>\n",
       "      <td>59</td>\n",
       "      <td>Annika Hoffman_Naoma Fritts@OOP.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Toshiko Hillyard</td>\n",
       "      <td>Cloud</td>\n",
       "      <td>12899</td>\n",
       "      <td>62</td>\n",
       "      <td>Margene Moores_Marylee Capasso@DB.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Male</td>\n",
       "      <td>Celeste Lollis</td>\n",
       "      <td>PF</td>\n",
       "      <td>21267</td>\n",
       "      <td>45</td>\n",
       "      <td>Jeannetta Golden_Jenna Montague@DSA.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Elenore Choy</td>\n",
       "      <td>DB</td>\n",
       "      <td>32877</td>\n",
       "      <td>29</td>\n",
       "      <td>Billi Clore_Mitzi Seldon@DB.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Male</td>\n",
       "      <td>Sheryll Towler</td>\n",
       "      <td>DSA</td>\n",
       "      <td>41487</td>\n",
       "      <td>41</td>\n",
       "      <td>Claude Panos_Judie Chipps@OOP.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>28</td>\n",
       "      <td>Female</td>\n",
       "      <td>Celeste Lollis</td>\n",
       "      <td>DB</td>\n",
       "      <td>9952416</td>\n",
       "      <td>59</td>\n",
       "      <td>Gonzalo Ferebee_Jalisa Swenson@DB.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Somer Stoecker</td>\n",
       "      <td>Cloud</td>\n",
       "      <td>9962277</td>\n",
       "      <td>84</td>\n",
       "      <td>Clementina Menke_Paris Hutton@OOP.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>28</td>\n",
       "      <td>Male</td>\n",
       "      <td>Tamera Blakley</td>\n",
       "      <td>DSA</td>\n",
       "      <td>9971217</td>\n",
       "      <td>26</td>\n",
       "      <td>Anna Santos_Claude Panos@PF.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>28</td>\n",
       "      <td>Female</td>\n",
       "      <td>Tamera Blakley</td>\n",
       "      <td>MVC</td>\n",
       "      <td>9982779</td>\n",
       "      <td>71</td>\n",
       "      <td>Toshiko Hillyard_Anna Santos@DSA.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>Jenna Montague</td>\n",
       "      <td>PF</td>\n",
       "      <td>9991617</td>\n",
       "      <td>23</td>\n",
       "      <td>Anna Santos_Jenna Montague@DB.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender              name course     roll  marks  \\\n",
       "0     28  Female   Hubert Oliveras     DB     2984     59   \n",
       "1     29  Female  Toshiko Hillyard  Cloud    12899     62   \n",
       "2     28    Male    Celeste Lollis     PF    21267     45   \n",
       "3     29  Female      Elenore Choy     DB    32877     29   \n",
       "4     28    Male    Sheryll Towler    DSA    41487     41   \n",
       "..   ...     ...               ...    ...      ...    ...   \n",
       "995   28  Female    Celeste Lollis     DB  9952416     59   \n",
       "996   29  Female    Somer Stoecker  Cloud  9962277     84   \n",
       "997   28    Male    Tamera Blakley    DSA  9971217     26   \n",
       "998   28  Female    Tamera Blakley    MVC  9982779     71   \n",
       "999   29  Female    Jenna Montague     PF  9991617     23   \n",
       "\n",
       "                                       email  \n",
       "0        Annika Hoffman_Naoma Fritts@OOP.com  \n",
       "1      Margene Moores_Marylee Capasso@DB.com  \n",
       "2    Jeannetta Golden_Jenna Montague@DSA.com  \n",
       "3            Billi Clore_Mitzi Seldon@DB.com  \n",
       "4          Claude Panos_Judie Chipps@OOP.com  \n",
       "..                                       ...  \n",
       "995    Gonzalo Ferebee_Jalisa Swenson@DB.com  \n",
       "996    Clementina Menke_Paris Hutton@OOP.com  \n",
       "997          Anna Santos_Claude Panos@PF.com  \n",
       "998     Toshiko Hillyard_Anna Santos@DSA.com  \n",
       "999        Anna Santos_Jenna Montague@DB.com  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show Pandase Data Frame from sparkDataframe\n",
    "student_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0ba73",
   "metadata": {},
   "source": [
    "## Using RDDs texFile Reading storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "77a1474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data = spark.sparkContext.textFile(\"./data/data.txt\")\n",
    "result_rdd = rdd_data.flatMap(lambda line: line.split(\" \"))\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a, b: a + b)\\\n",
    "    .sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ca4c5bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 12),\n",
       " ('of', 7),\n",
       " ('a', 7),\n",
       " ('in', 5),\n",
       " ('distributed', 5),\n",
       " ('Spark', 4),\n",
       " ('is', 3),\n",
       " ('as', 3),\n",
       " ('API', 3),\n",
       " ('on', 3)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f81687",
   "metadata": {},
   "source": [
    "### Using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "36b43d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df_textData = spark.read.text(\"./data/data.txt\")\n",
    "result_textData = df_textData.selectExpr(\"explode(split(value, ' ')) as word\") \\\n",
    "    .groupBy(\"word\").count().orderBy(desc(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "704eefea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       word|count|\n",
      "+-----------+-----+\n",
      "|        the|   12|\n",
      "|          a|    7|\n",
      "|         of|    7|\n",
      "|         in|    5|\n",
      "|distributed|    5|\n",
      "|      Spark|    4|\n",
      "|        API|    3|\n",
      "|         as|    3|\n",
      "|        RDD|    3|\n",
      "|         is|    3|\n",
      "|    Dataset|    3|\n",
      "|         on|    3|\n",
      "|        its|    2|\n",
      "|       that|    2|\n",
      "|        The|    2|\n",
      "|  MapReduce|    2|\n",
      "|       API.|    2|\n",
      "|        and|    2|\n",
      "|   function|    2|\n",
      "|    cluster|    2|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_textData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4236ec5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='the', count=12),\n",
       " Row(word='a', count=7),\n",
       " Row(word='of', count=7),\n",
       " Row(word='in', count=5),\n",
       " Row(word='distributed', count=5),\n",
       " Row(word='Spark', count=4),\n",
       " Row(word='RDD', count=3),\n",
       " Row(word='API', count=3),\n",
       " Row(word='as', count=3),\n",
       " Row(word='is', count=3)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_textData.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f0691",
   "metadata": {},
   "source": [
    "## Read CSV file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2765a0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,category,quantity,price\n",
      "1,iPhone 12,Electronics,10,899.99\n",
      "2,Nike Air Max 90,Clothing,25,119.99\n",
      "3,KitchenAid Stand Mixer,Home Appliances,5,299.99\n",
      "4,The Great Gatsby,Books,50,12.99\n",
      "5,L'Oreal Paris Mascara,Beauty,100,9.99\n",
      "6,Yoga Mat,Sports,30,29.99\n",
      "7,Samsung 4K Smart TV,Electronics,8,799.99\n",
      "8,Levi's Jeans,Clothing,15,49.99\n",
      "9,Dyson Vacuum Cleaner,Home Appliances,3,399.99\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "head -10 ./data/products.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d906b",
   "metadata": {},
   "source": [
    "## Read CSV with header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "83296d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "product_df = spark.read.csv(csv_file_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d9909fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "product_df.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "product_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0104410d",
   "metadata": {},
   "source": [
    "### Read CSV with an explicit schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bffd9f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1c017e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(name=\"id\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"name\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"category\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"quantity\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"price\", dataType=DoubleType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Read CSV file into DataFrame with schema definition\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "product_df1 = spark.read.csv(csv_file_path, header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d23d7a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "product_df1.printSchema()\n",
    "\n",
    "# Display content of DataFrame\n",
    "product_df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846ef8f",
   "metadata": {},
   "source": [
    "#### Read CSV with using inferSchema Atomatically define schema type by using InferSchema=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7e46c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file into DataFrame with inferSchema\n",
    "csv_file_path = \"./data/products.csv\"\n",
    "product_df2 = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "44c61596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+--------------------+---------------+--------+------+\n",
      "| id|                name|       category|quantity| price|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "|  1|           iPhone 12|    Electronics|      10|899.99|\n",
      "|  2|     Nike Air Max 90|       Clothing|      25|119.99|\n",
      "|  3|KitchenAid Stand ...|Home Appliances|       5|299.99|\n",
      "|  4|    The Great Gatsby|          Books|      50| 12.99|\n",
      "|  5|L'Oreal Paris Mas...|         Beauty|     100|  9.99|\n",
      "+---+--------------------+---------------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "product_df2.printSchema()\n",
    "# Display content of DataFrame\n",
    "product_df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb3beec",
   "metadata": {},
   "source": [
    "### Read JSON file into DataFrame\n",
    "### Single Line JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0cfb0a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":1,\"name\":\"iPhone 12\",\"category\":\"Electronics\",\"quantity\":10,\"price\":899.99}\n",
      "{\"id\":2,\"name\":\"Nike Air Max 90\",\"category\":\"Clothing\",\"quantity\":25,\"price\":119.99}\n",
      "{\"id\":3,\"name\":\"KitchenAid Stand Mixer\",\"category\":\"Home Appliances\",\"quantity\":5,\"price\":299.99}\n",
      "{\"id\":4,\"name\":\"The Great Gatsby\",\"category\":\"Books\",\"quantity\":50,\"price\":12.99}\n",
      "{\"id\":5,\"name\":\"L'Oreal Paris Mascara\",\"category\":\"Beauty\",\"quantity\":100,\"price\":9.99}\n",
      "{\"id\":6,\"name\":\"Yoga Mat\",\"category\":\"Sports\",\"quantity\":30,\"price\":29.99}\n",
      "{\"id\":7,\"name\":\"Samsung 4K Smart TV\",\"category\":\"Electronics\",\"quantity\":8,\"price\":799.99}\n",
      "{\"id\":8,\"name\":\"Levi's Jeans\",\"category\":\"Clothing\",\"quantity\":15,\"price\":49.99}\n",
      "{\"id\":9,\"name\":\"Dyson Vacuum Cleaner\",\"category\":\"Home Appliances\",\"quantity\":3,\"price\":399.99}\n",
      "{\"id\":10,\"name\":\"Harry Potter Series\",\"category\":\"Books\",\"quantity\":20,\"price\":15.99}\n",
      "{\"id\":11,\"name\":\"MAC Lipstick\",\"category\":\"Beauty\",\"quantity\":75,\"price\":16.99}\n",
      "{\"id\":12,\"name\":\"Adidas Running Shoes\",\"category\":\"Sports\",\"quantity\":22,\"price\":59.99}\n",
      "{\"id\":13,\"name\":\"PlayStation 5\",\"category\":\"Electronics\",\"quantity\":12,\"price\":499.99}\n",
      "{\"id\":14,\"name\":\"Hooded Sweatshirt\",\"category\":\"Clothing\",\"quantity\":10,\"price\":34.99}\n",
      "{\"id\":15,\"name\":\"Coffee Maker\",\"category\":\"Home Appliances\",\"quantity\":7,\"price\":89.99}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -15 ./data/products_singleline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f22747d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read single line JSON\n",
    "# Each row is a JSON record, records are separated by new line\n",
    "json_file_path = \"./data/products_singleline.json\"\n",
    "json_data_df = spark.read.json(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "90193df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "|         Sports|  6|            Yoga Mat| 29.99|      30|\n",
      "|    Electronics|  7| Samsung 4K Smart TV|799.99|       8|\n",
      "|       Clothing|  8|        Levi's Jeans| 49.99|      15|\n",
      "|Home Appliances|  9|Dyson Vacuum Cleaner|399.99|       3|\n",
      "|          Books| 10| Harry Potter Series| 15.99|      20|\n",
      "|         Beauty| 11|        MAC Lipstick| 16.99|      75|\n",
      "|         Sports| 12|Adidas Running Shoes| 59.99|      22|\n",
      "|    Electronics| 13|       PlayStation 5|499.99|      12|\n",
      "|       Clothing| 14|   Hooded Sweatshirt| 34.99|      10|\n",
      "|Home Appliances| 15|        Coffee Maker| 89.99|       7|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "json_data_df.printSchema()\n",
    "# Display content of DataFrame\n",
    "json_data_df.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13f0c1b",
   "metadata": {},
   "source": [
    "## Multi-lines JSON and hass of Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "252fb2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"name\": \"iPhone 12\",\n",
      "    \"category\": \"Electronics\",\n",
      "    \"quantity\": 10,\n",
      "    \"price\": 899.99\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"name\": \"Nike Air Max 90\",\n",
      "    \"category\": \"Clothing\",\n",
      "    \"quantity\": 25,\n",
      "    \"price\": 119.99\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"name\": \"KitchenAid Stand Mixer\",\n",
      "    \"category\": \"Home Appliances\",\n",
      "    \"quantity\": 5,\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -20 ./data/products_multiline.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "adb6fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read multi-line JSON\n",
    "# JSON is an array of record, records are separated by a comma.\n",
    "# each record is defined in multiple lines\n",
    "json_file_path = \"./data/products_multiline.json\"\n",
    "json_data_df = spark.read.json(json_file_path, multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a9527bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "json_data_df.printSchema()\n",
    "# Display content of DataFrame\n",
    "json_data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e37242d",
   "metadata": {},
   "source": [
    "## Save in Parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "c5e623d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe into parquet file First time execute second time will fail\n",
    "parquet_file_path = \"./data/products.parquet\"\n",
    "json_data_df.write.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad5ed0",
   "metadata": {},
   "source": [
    "## Read parquet file into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "53709b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Parquet fiels \n",
    "parquet_df = spark.read.parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "09886de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      "\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|       category| id|                name| price|quantity|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "|    Electronics|  1|           iPhone 12|899.99|      10|\n",
      "|       Clothing|  2|     Nike Air Max 90|119.99|      25|\n",
      "|Home Appliances|  3|KitchenAid Stand ...|299.99|       5|\n",
      "|          Books|  4|    The Great Gatsby| 12.99|      50|\n",
      "|         Beauty|  5|L'Oreal Paris Mas...|  9.99|     100|\n",
      "+---------------+---+--------------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "parquet_df.printSchema()\n",
    "# Display content of DataFrame\n",
    "parquet_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03341c4d",
   "metadata": {},
   "source": [
    "## 2. DataFrame Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3e543a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,name,category,quantity,price\n",
      "1,iPhone,Electronics,10,899.99\n",
      "2,Macbook,Electronics,5,1299.99\n",
      "3,iPad,Electronics,15,499.99\n",
      "4,Samsung TV,Electronics,8,799.99\n",
      "5,LG TV,Electronics,10,699.99\n",
      "6,Nike Shoes,Clothing,30,99.99\n",
      "7,Adidas Shoes,Clothing,25,89.99\n",
      "8,Sony Headphones,Electronics,12,149.99\n",
      "9,Beats Headphones,Electronics,20,199.99\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -10 ./data/stocks.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "b6023f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic data into a DataFrame\n",
    "data_file_path = \"./data/stocks.txt\"\n",
    "stocks_data_df = spark.read.csv(data_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "53460290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stocks_data_df.printSchema()\n",
    "stocks_data_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88eff13",
   "metadata": {},
   "source": [
    "### Select: Choose specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d1c2906f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Columns:\n",
      "+---+----------------+-------+\n",
      "| id|            name|  price|\n",
      "+---+----------------+-------+\n",
      "|  1|          iPhone| 899.99|\n",
      "|  2|         Macbook|1299.99|\n",
      "|  3|            iPad| 499.99|\n",
      "|  4|      Samsung TV| 799.99|\n",
      "|  5|           LG TV| 699.99|\n",
      "|  6|      Nike Shoes|  99.99|\n",
      "|  7|    Adidas Shoes|  89.99|\n",
      "|  8| Sony Headphones| 149.99|\n",
      "|  9|Beats Headphones| 199.99|\n",
      "| 10|    Dining Table| 249.99|\n",
      "+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns \n",
    "selected_columns = stocks_data_df.select(\"id\", \"name\", \"price\")\n",
    "print(\"Selected Columns:\")\n",
    "selected_columns.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79e732",
   "metadata": {},
   "source": [
    "### Filter: Apply conditions to filter rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e2199b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data: 14\n",
      "+---+----------------+-----------+--------+------+\n",
      "| id|            name|   category|quantity| price|\n",
      "+---+----------------+-----------+--------+------+\n",
      "|  6|      Nike Shoes|   Clothing|      30| 99.99|\n",
      "|  7|    Adidas Shoes|   Clothing|      25| 89.99|\n",
      "|  9|Beats Headphones|Electronics|      20|199.99|\n",
      "| 12|          Apples|       Food|     100|   0.5|\n",
      "| 13|         Bananas|       Food|     150|  0.25|\n",
      "| 14|         Oranges|       Food|     120|  0.75|\n",
      "| 15|  Chicken Breast|       Food|      50|  3.99|\n",
      "| 16|   Salmon Fillet|       Food|      30|  5.99|\n",
      "| 19|        Yoga Mat|     Sports|      20| 19.99|\n",
      "| 24|      Laptop Bag|Accessories|      25| 29.99|\n",
      "| 25|        Backpack|Accessories|      30| 24.99|\n",
      "| 28|           Jeans|   Clothing|      30| 59.99|\n",
      "| 29|         T-shirt|   Clothing|      50| 14.99|\n",
      "| 30|        Sneakers|   Clothing|      40| 79.99|\n",
      "+---+----------------+-----------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows based on a condition\n",
    "filtered_data = stocks_data_df.filter(stocks_data_df.quantity > 15)\n",
    "print(\"Filtered Data:\", filtered_data.count())\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc167d6",
   "metadata": {},
   "source": [
    "**GroupBy: Group data based on specific columns**\n",
    "\n",
    "**Aggregations: Perform functions like sum, average, etc., on grouped data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5546c893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped and Aggregated Data:\n",
      "+-----------+-------------+------------------+\n",
      "|   category|sum(quantity)|        avg(price)|\n",
      "+-----------+-------------+------------------+\n",
      "|Electronics|           98| 586.6566666666665|\n",
      "|       Food|          450|2.2960000000000003|\n",
      "|   Clothing|          200|  99.2757142857143|\n",
      "|  Furniture|           41|            141.99|\n",
      "|     Sports|           35|             34.99|\n",
      "|Accessories|           55|             27.49|\n",
      "+-----------+-------------+------------------+\n",
      "\n",
      "Count Row no: 6\n"
     ]
    }
   ],
   "source": [
    "# GroupBy and Aggregations\n",
    "grouped_data = stocks_data_df.groupBy(\"category\").agg({\"quantity\": \"sum\", \"price\": \"avg\"})\n",
    "print(\"Grouped and Aggregated Data:\")\n",
    "grouped_data.show()\n",
    "\n",
    "print(\"Count Row no:\",grouped_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f999ea",
   "metadata": {},
   "source": [
    "### Join: Combine multiple DataFrames based on specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f4d0f8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined Data:\n",
      "+---+----------------+-----------+--------+-------+-----------+\n",
      "| id|            name|   category|quantity|  price|   category|\n",
      "+---+----------------+-----------+--------+-------+-----------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99|Electronics|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|Electronics|\n",
      "|  3|            iPad|Electronics|      15| 499.99|Electronics|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|Electronics|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|Electronics|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99|   Clothing|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|   Clothing|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|Electronics|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|Electronics|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|  Furniture|\n",
      "| 11|      Study Desk|  Furniture|       8| 149.99|  Furniture|\n",
      "| 12|          Apples|       Food|     100|    0.5|       Food|\n",
      "| 13|         Bananas|       Food|     150|   0.25|       Food|\n",
      "| 14|         Oranges|       Food|     120|   0.75|       Food|\n",
      "| 15|  Chicken Breast|       Food|      50|   3.99|       Food|\n",
      "+---+----------------+-----------+--------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join with another DataFrame\n",
    "df2 = stocks_data_df.select(\"id\", \"category\").limit(15)\n",
    "joined_data = stocks_data_df.join(df2, \"id\", \"inner\")\n",
    "print(\"Joined Data:\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ba6ef",
   "metadata": {},
   "source": [
    "### Sort: Arrange rows based on one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "683379a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Data:\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| id|          name|   category|quantity|price|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "| 13|       Bananas|       Food|     150| 0.25|\n",
      "| 12|        Apples|       Food|     100|  0.5|\n",
      "| 14|       Oranges|       Food|     120| 0.75|\n",
      "| 15|Chicken Breast|       Food|      50| 3.99|\n",
      "| 16| Salmon Fillet|       Food|      30| 5.99|\n",
      "| 29|       T-shirt|   Clothing|      50|14.99|\n",
      "| 19|      Yoga Mat|     Sports|      20|19.99|\n",
      "| 25|      Backpack|Accessories|      30|24.99|\n",
      "| 24|    Laptop Bag|Accessories|      25|29.99|\n",
      "| 20|  Dumbbell Set|     Sports|      15|49.99|\n",
      "+---+--------------+-----------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column\n",
    "sorted_data = stocks_data_df.orderBy(\"price\")\n",
    "print(\"Sorted Data:\")\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "394a27b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Data Descending:\n",
      "+---+----------------+-----------+--------+-------+\n",
      "| id|            name|   category|quantity|  price|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "|  2|         Macbook|Electronics|       5|1299.99|\n",
      "|  1|          iPhone|Electronics|      10| 899.99|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|\n",
      "|  5|           LG TV|Electronics|      10| 699.99|\n",
      "| 26|          Camera|Electronics|      10| 599.99|\n",
      "|  3|            iPad|Electronics|      15| 499.99|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99|\n",
      "| 17|  Leather Jacket|   Clothing|      15| 199.99|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99|\n",
      "| 18|     Winter Coat|   Clothing|      10| 149.99|\n",
      "+---+----------------+-----------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by a column desc\n",
    "from pyspark.sql.functions import col, desc\n",
    "sorted_data = stocks_data_df.orderBy(col(\"price\").desc(), col(\"id\").desc())\n",
    "print(\"Sorted Data Descending:\")\n",
    "sorted_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec17bc0f",
   "metadata": {},
   "source": [
    "## Distinct: Get unique rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e6655310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Product Categories:\n",
      "+-----------+\n",
      "|   category|\n",
      "+-----------+\n",
      "|Electronics|\n",
      "|       Food|\n",
      "|   Clothing|\n",
      "|  Furniture|\n",
      "|     Sports|\n",
      "|Accessories|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get distinct product category\n",
    "distinct_rows = stocks_data_df.select(\"category\").distinct()\n",
    "print(\"Distinct Product Categories:\")\n",
    "distinct_rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd591c",
   "metadata": {},
   "source": [
    "### Drop: Remove specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e9825f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped Columns:\n",
      "+---+----------------+-------+\n",
      "| id|            name|  price|\n",
      "+---+----------------+-------+\n",
      "|  1|          iPhone| 899.99|\n",
      "|  2|         Macbook|1299.99|\n",
      "|  3|            iPad| 499.99|\n",
      "|  4|      Samsung TV| 799.99|\n",
      "|  5|           LG TV| 699.99|\n",
      "|  6|      Nike Shoes|  99.99|\n",
      "|  7|    Adidas Shoes|  89.99|\n",
      "|  8| Sony Headphones| 149.99|\n",
      "|  9|Beats Headphones| 199.99|\n",
      "| 10|    Dining Table| 249.99|\n",
      "+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop columns\n",
    "dropped_columns = stocks_data_df.drop(\"quantity\", \"category\")\n",
    "print(\"Dropped Columns:\")\n",
    "dropped_columns.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54853d1a",
   "metadata": {},
   "source": [
    "## WithColumn: Add new calculated columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3fd4ee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with New Column:\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "| id|            name|   category|quantity|  price|revenue|\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "|  1|          iPhone|Electronics|      10| 899.99| 8999.9|\n",
      "|  2|         Macbook|Electronics|       5|1299.99|6499.95|\n",
      "|  3|            iPad|Electronics|      15| 499.99|7499.85|\n",
      "|  4|      Samsung TV|Electronics|       8| 799.99|6399.92|\n",
      "|  5|           LG TV|Electronics|      10| 699.99| 6999.9|\n",
      "|  6|      Nike Shoes|   Clothing|      30|  99.99| 2999.7|\n",
      "|  7|    Adidas Shoes|   Clothing|      25|  89.99|2249.75|\n",
      "|  8| Sony Headphones|Electronics|      12| 149.99|1799.88|\n",
      "|  9|Beats Headphones|Electronics|      20| 199.99| 3999.8|\n",
      "| 10|    Dining Table|  Furniture|      10| 249.99| 2499.9|\n",
      "+---+----------------+-----------+--------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add a new calculated column\n",
    "df_with_new_column = stocks_data_df.withColumn(\"revenue\", stocks_data_df.quantity * stocks_data_df.price)\n",
    "print(\"DataFrame with New Column:\")\n",
    "df_with_new_column.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07384e8b",
   "metadata": {},
   "source": [
    "### Alias: Rename columns for better readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4571ce0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with Aliased Column:\n",
      "+---+----------------+-------------+--------+-------------+\n",
      "| id|            name|Category_Name|quantity|product_price|\n",
      "+---+----------------+-------------+--------+-------------+\n",
      "|  1|          iPhone|  Electronics|      10|       899.99|\n",
      "|  2|         Macbook|  Electronics|       5|      1299.99|\n",
      "|  3|            iPad|  Electronics|      15|       499.99|\n",
      "|  4|      Samsung TV|  Electronics|       8|       799.99|\n",
      "|  5|           LG TV|  Electronics|      10|       699.99|\n",
      "|  6|      Nike Shoes|     Clothing|      30|        99.99|\n",
      "|  7|    Adidas Shoes|     Clothing|      25|        89.99|\n",
      "|  8| Sony Headphones|  Electronics|      12|       149.99|\n",
      "|  9|Beats Headphones|  Electronics|      20|       199.99|\n",
      "| 10|    Dining Table|    Furniture|      10|       249.99|\n",
      "+---+----------------+-------------+--------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename columns using alias\n",
    "df_with_alias = stocks_data_df.withColumnRenamed(\"price\", \"product_price\").withColumnRenamed('category', 'Category_Name')\n",
    "print(\"DataFrame with Aliased Column:\")\n",
    "df_with_alias.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f584d68d",
   "metadata": {},
   "source": [
    "## 3. Spark SQL Operation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d1a8db50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name,age,gender,salary\n",
      "John Doe,30,Male,50000\n",
      "Jane Smith,25,Female,45000\n",
      "David Johnson,35,Male,60000\n",
      "Emily Davis,28,Female,52000\n",
      "Michael Wilson,40,Male,75000\n",
      "Sarah Brown,32,Female,58000\n",
      "Robert Lee,29,Male,51000\n",
      "Lisa Garcia,27,Female,49000\n",
      "James Martinez,38,Male,70000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head -10 ./data/persons.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5aeae582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the synthetic data into a DataFrame\n",
    "data_file_path = \"./data/persons.csv\"\n",
    "person_data_df = spark.read.csv(data_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "219b35e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "Initial DataFrame:\n",
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|        Jane Smith| 25|Female| 45000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "+------------------+---+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display schema of DataFrame\n",
    "person_data_df.printSchema()\n",
    "# Show the initial DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "person_data_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3770d",
   "metadata": {},
   "source": [
    "## Register the DataFrame as a Temporary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e93e76af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a Temporary Table\n",
    "person_data_df.createOrReplaceTempView(\"person_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338b487",
   "metadata": {},
   "source": [
    "## Now, We Can Perform SQL-like Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c52dc26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|          John Doe| 30|  Male| 50000|\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|       Emily Davis| 28|Female| 52000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|        Robert Lee| 29|  Male| 51000|\n",
      "|       Lisa Garcia| 27|Female| 49000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|Jennifer Rodriguez| 26|Female| 47000|\n",
      "|  William Anderson| 33|  Male| 62000|\n",
      "|   Karen Hernandez| 31|Female| 55000|\n",
      "|Christopher Taylor| 37|  Male| 69000|\n",
      "|     Matthew Davis| 36|  Male| 67000|\n",
      "|    Patricia White| 29|Female| 50000|\n",
      "|     Daniel Miller| 34|  Male| 64000|\n",
      "| Elizabeth Jackson| 30|Female| 52000|\n",
      "|     Joseph Harris| 28|  Male| 53000|\n",
      "|      Linda Martin| 39|Female| 71000|\n",
      "+------------------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all rows where age is greater than 25\n",
    "result = spark.sql(\"SELECT * FROM person_table WHERE age > 25\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "98a839d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|gender|avg_salary|\n",
      "+------+----------+\n",
      "|Female|   52300.0|\n",
      "|  Male|   62100.0|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the average salary by gender\n",
    "avg_salary_by_gender = spark.sql(\"SELECT gender, AVG(salary) as avg_salary FROM person_table GROUP BY gender\")\n",
    "avg_salary_by_gender.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abbb71",
   "metadata": {},
   "source": [
    "## Creating and managing temporary views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bacf08e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "person_data_df.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "fd00fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+------+------+\n",
      "|              name|age|gender|salary|\n",
      "+------------------+---+------+------+\n",
      "|     David Johnson| 35|  Male| 60000|\n",
      "|    Michael Wilson| 40|  Male| 75000|\n",
      "|       Sarah Brown| 32|Female| 58000|\n",
      "|    James Martinez| 38|  Male| 70000|\n",
      "|  William Anderson| 33|  Male| 62000|\n",
      "|   Karen Hernandez| 31|Female| 55000|\n",
      "|Christopher Taylor| 37|  Male| 69000|\n",
      "|     Matthew Davis| 36|  Male| 67000|\n",
      "|     Daniel Miller| 34|  Male| 64000|\n",
      "|      Linda Martin| 39|Female| 71000|\n",
      "+------------------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query the temporary view\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 30\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "14ccb10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a temporary view exists\n",
    "view_exists = spark.catalog.tableExists(\"people\")\n",
    "view_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "940cc837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop a temporary view\n",
    "spark.catalog.dropTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "32b43558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if a temporary view exists\n",
    "view_exists = spark.catalog.tableExists(\"people\")\n",
    "view_exists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ccb229",
   "metadata": {},
   "source": [
    "## SQL Subquries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "643d5cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|   John|\n",
      "|  2|  Alice|\n",
      "|  3|    Bob|\n",
      "|  4|  Emily|\n",
      "|  5|  David|\n",
      "|  6|  Sarah|\n",
      "|  7|Michael|\n",
      "|  8|   Lisa|\n",
      "|  9|William|\n",
      "+---+-------+\n",
      "\n",
      "+----------+---+------+\n",
      "|department| id|salary|\n",
      "+----------+---+------+\n",
      "|        HR|  1| 60000|\n",
      "|        HR|  2| 55000|\n",
      "|        HR|  3| 58000|\n",
      "|        IT|  4| 70000|\n",
      "|        IT|  5| 72000|\n",
      "|        IT|  6| 68000|\n",
      "|     Sales|  7| 75000|\n",
      "|     Sales|  8| 78000|\n",
      "|     Sales|  9| 77000|\n",
      "+----------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrames\n",
    "employee_data = [\n",
    "    (1, \"John\"), (2, \"Alice\"), (3, \"Bob\"), (4, \"Emily\"),\n",
    "    (5, \"David\"), (6, \"Sarah\"), (7, \"Michael\"), (8, \"Lisa\"),\n",
    "    (9, \"William\")\n",
    "]\n",
    "employees = spark.createDataFrame(employee_data, [\"id\", \"name\"])\n",
    "\n",
    "salary_data = [\n",
    "    (\"HR\", 1, 60000), (\"HR\", 2, 55000), (\"HR\", 3, 58000),\n",
    "    (\"IT\", 4, 70000), (\"IT\", 5, 72000), (\"IT\", 6, 68000),\n",
    "    (\"Sales\", 7, 75000), (\"Sales\", 8, 78000), (\"Sales\", 9, 77000)\n",
    "]\n",
    "salaries = spark.createDataFrame(salary_data, [\"department\", \"id\", \"salary\"])\n",
    "\n",
    "employees.show()\n",
    "\n",
    "salaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "22776672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as temporary views\n",
    "employees.createOrReplaceTempView(\"employees\")\n",
    "salaries.createOrReplaceTempView(\"salaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "0e3aa127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  Emily|\n",
      "|  David|\n",
      "|Michael|\n",
      "|   Lisa|\n",
      "|William|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subquery to find employees with salaries above average\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT name\n",
    "    FROM employees\n",
    "    WHERE id IN (\n",
    "        SELECT id\n",
    "        FROM salaries\n",
    "        WHERE salary > (SELECT AVG(salary) FROM salaries)\n",
    "    )\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cbdb3a",
   "metadata": {},
   "source": [
    "### Working with Window Functions in PySpark\n",
    "\n",
    "#### Reference\n",
    "https://www.analyticsvidhya.com/blog/2024/03/working-with-window-functions-in-pyspark/#:~:text=Approach%20for%20PySpark%20code,sal%E2%80%9D%20column%20in%20descending%20order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "980ec854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "c5875706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------+\n",
      "|department| id|salary|   name|\n",
      "+----------+---+------+-------+\n",
      "|        HR|  1| 60000|   John|\n",
      "|        HR|  2| 55000|  Alice|\n",
      "|        HR|  3| 58000|    Bob|\n",
      "|        IT|  4| 70000|  Emily|\n",
      "|        IT|  5| 72000|  David|\n",
      "|        IT|  6| 68000|  Sarah|\n",
      "|     Sales|  7| 75000|Michael|\n",
      "|     Sales|  8| 78000|   Lisa|\n",
      "|     Sales|  9| 77000|William|\n",
      "+----------+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_salary = spark.sql(\"\"\"\n",
    "    select  salaries.*, employees.name\n",
    "    from salaries \n",
    "    left join employees on salaries.id = employees.id\n",
    "\"\"\")\n",
    "\n",
    "employee_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e9ad0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "57d1b2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+-------+----+\n",
      "|department| id|salary|   name|rank|\n",
      "+----------+---+------+-------+----+\n",
      "|        HR|  1| 60000|   John|   1|\n",
      "|        HR|  3| 58000|    Bob|   2|\n",
      "|        HR|  2| 55000|  Alice|   3|\n",
      "|        IT|  5| 72000|  David|   1|\n",
      "|        IT|  4| 70000|  Emily|   2|\n",
      "|        IT|  6| 68000|  Sarah|   3|\n",
      "|     Sales|  8| 78000|   Lisa|   1|\n",
      "|     Sales|  9| 77000|William|   2|\n",
      "|     Sales|  7| 75000|Michael|   3|\n",
      "+----------+---+------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the rank of employees within each department based on salary\n",
    "employee_salary.withColumn(\"rank\", F.rank().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "be57b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6113d83",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "### pySpark Tutorials \n",
    "\n",
    "https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "\n",
    "https://medium.com/codex/pyspark-for-begineers-part-2-pyspark-dataframe-60008da53e30\n",
    "\n",
    "https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm\n",
    "\n",
    "https://blog.devgenius.io/pyspark-for-begineers-part-3-pyspark-dataframe-db02f0fcd275\n",
    "\n",
    "\n",
    "### Big Data Hadoop Setup & Configuration and Command Line Data processing \n",
    "\n",
    "https://medium.com/@jonty2245/install-apache-hadoop-on-windows-11-a-beginners-guide-45a149f47f8a\n",
    "\n",
    "https://intellipaat.com/blog/tutorial/hadoop-tutorial/hdfs-operations/\n",
    "\n",
    "https://www.projectpro.io/hadoop-tutorial/hadoop-hdfs-commands\n",
    "\n",
    "https://www.geeksforgeeks.org/hdfs-commands/\n",
    "\n",
    "https://medium.com/@ashwin_kumar_/hadoop-hdfs-commands-with-examples-and-usage-570038cbef07\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
